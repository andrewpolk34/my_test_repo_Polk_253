---
title: "For Real, or Fraud? Classification Modeling With Online Job Postings"
author: "Liam Edwards-Gaherty, Matthew Plisko, Andrew Polk, Henry Smith"
date: "4/23/2020"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

```{r, echo=FALSE}
#plotting and exploring
library(tidyverse) #for plotting and summarizing
library(GGally) #for nice scatterplot matrix 
library(ggridges) #for joy/ridge plots
library(corrplot) #for basic correlation matrix plot
library(naniar) #for exploring missing values
library(pdp) #for partial dependence plots, MARS models
library(rpart.plot) #for plotting decision trees
library(vip) #for importance plots
library(pROC) #for ROC curves
library(plotROC) #for plotting ROC curves

#making things look nice
library(lubridate) #for nice dates
library(knitr) #for nice tables
library(scales) #for nice labels on graphs
library(gridExtra) #for arranging plots
library(broom) #for nice model output
library(janitor) #for nice names

#data
library(ISLR) #for data
library(moderndive) #for data
library(rattle) #weather data
library(fivethirtyeight) #candy data

#modeling
library(rsample) #for splitting data
library(recipes) #for keeping track of transformations
library(caret) #for modeling
library(leaps) #for variable selection
library(glmnet) #for LASSO
library(earth) #for MARS models
library(rpart) #for decision trees
library(randomForest) #for bagging and random forests

theme_set(theme_minimal())
```

```{r, echo=FALSE}
jobs <- read_csv("fake_job_postings.csv") %>%
  mutate(employment_type_new = fct_recode(employment_type, full_time = "Full-time"))
```

```{r, echo=FALSE, include=FALSE}
jobs <- jobs %>%
add_n_miss(label = "n_miss")
jobs <- jobs %>%
mutate_if(is.character, replace_na, replace = "Missing") %>%
mutate(comp_prof_numchar = nchar(company_profile, type = "char", allowNA = FALSE, keepNA =NA),description_numchar = nchar(description, type = "char", allowNA = FALSE, keepNA =NA)) %>%
mutate(domestic = sapply(strsplit(jobs$location, split = ',', fixed=TRUE), function(x) (x[1]))) %>% 
mutate(domestic = ifelse(domestic != "US" | is.na(domestic),0,1))

#jobs <- jobs %>%
#  group_by(description,requirements) %>%
#  mutate(count = (ifelse(is.na(description)==FALSE & is.na(requirements)==FALSE,n(),1000))) %>% replace_with_na(replace = list(count = 1000))

#divide into training and testing
set.seed(253) #first set the seed!
jobs_split <- initial_split(jobs, prop = .7, 
                             strata = fraudulent)
jobs_train <- training(jobs_split)
jobs_test <- testing(jobs_split)

#distribution of response for the training data
table(jobs_train$fraudulent) %>% prop.table()

#distribution of response for testing data
table(jobs_test$fraudulent) %>% prop.table()

```

# Introduction and Research Questions

For our final project, we used a dataset from Kaggle containing 17,800 online job postings, of which about 5% are fraudulent, fake job postings. Our research questions are:

**What are some universal signs of a fraudulent job posting? **

**What classification model best identifies the fraudulent job postings in the dataset?**

In this report, we explain the data used, and we explore specific variables and their relationship with our response variable. Some variables that we found had significant relationships with our response variable include has_company_logo and description_numchar. We then summarize the models we built to predict fraudulent job postings, and we select our best models to test using test data. After selecting our best model, a random forest model, we observe that it accurately predicts fraudulent job postings 85% of the time on sampled down data, and we form conclusions about its effectiveness and interpretability.

# Data: Modifications and Challenges

Since only about 5% of the job postings are fraudulent, we need to develop a highly accurate classification model to identify the fraudulent postings at a statistically significant rate. The data from Kaggle contains 17,880 observations with 18 variables. The response variable is called **fraudulent**; it is a binary variable, with 0 signifying a legitimate job posting and 1 signifying a fraudulent job posting. 

```{r, echo=FALSE}
jobs %>%
  count(fraudulent)
```


We made some modifications and additions to the dataset, including:

- Replacing empty values with "Missing"
- Generating a variable, **n_miss_all**, to count to amount of missing categories for each job posting
- Generating variables that counted the amount of characters for the **company_profile** and **description** variables
- Generating a binary variable, **domestic**, that designated a job as either domestic (US) or foreign

Below is a description of key variables we use in the plots, tables, and models that follow:

| Variable | Definition |
|-----------|-----------|
| **fraudulent**| 1 if job post is fake, 0 otherwise |
|job_id| Numbers that signifies the unique job posting |
| title | Job title |
| location |  Where the job is located|
| department | Which department the job is posted in |
| salary_range | Catagorical variable with different levels of salaries |
| company_profile | Brief blurb that describes the company |
| description | Brief blurb that describes the job|
| requirements | Blurb that describes what is required for the job |
| benefits | A description of the benefits of the job |
|telecommuning | 1 if job is within the telecommunication field, 0 otherwise|
|has_company_logo | 1 if job posting has logo on posting, 0 otherwise|
| has_questions | 1 if job posting has a question in their posting, 0 otherwise|
| employment_type | Whether the position is full-time or part-time |
| required_education | What level of education is required for the job |
| industry | the industry where the job is posted |
| function | type of job |
|n_miss_all | Number of missing entries in the dataset |
|comp_prof_numchar| Number of characters in the company description |
|description_numchar| Number of characters in the description of the job |
| domestic | 1 if the job is in the U.S., 0 otherwise |

# Exploratory Plots
The plots below explore variables that we found had significant or interesting relationships with our response variable

**1.** This plot explores the required_experience variable as it pertains to the fraudulent variable. This plot shows that entry level jobs and executive jobs have the highest proportion of fraudulent job postings.
```{r, fig.width=9, echo=FALSE}
jobs_train %>%
  ggplot(aes(x = required_experience, fill = as.factor(fraudulent))) +
  geom_bar(position = "fill") +
  ggtitle("Fraudulent Versus Legitimate Job Postings, by Required Experience") +
  labs(x = "Required Experience", y = "Proportion", fill = "Fraudulent")
```

**2.** This plot explore the has_company_logo variable as it pertain to the fraudulent variable. The plot shows that job postings containing a company logo are much more likely to be legitimate postings.
```{r, echo=FALSE}
jobs_train %>%
  ggplot(aes(x = as.factor(has_company_logo))) +
  geom_bar(aes(fill = as.factor(fraudulent)), position = "fill") +
  ggtitle("Fraudulent Versus Legitimate Job Postings, by Inclusion of Company Logo") +
  labs(x = "Job Posting Has Company Logo", fill= "Fraudulent")
```

**3.** The graph below shows that a job posting is overwhelmingly likely to be legitimate if it contains both the company logo and has questions. While only 4.8% of the observations are fraudulent job postings, only 1.9% of observations containing both company logos and questions are fraudulent, showing the significance of this variable.
```{r, echo=FALSE, fig.height=5}
jobs_train %>%
  filter(has_company_logo == 1, has_questions == 1) %>%
  ggplot(aes(x = as.factor(fraudulent))) +
  geom_bar() +
  geom_text(stat='count', aes(label=..count.., vjust=-.5)) +
  ggtitle("Fraudulent Versus Legitimate Job Postings, by Inclusion of Company Logo and Questions") +
  labs(x = "Fraudulent?")
``` 

**4.** This plot filters for job location within the United States, showing that more than half of the job postings within the data are domestic. Domestic job postings are more likely to be fraudulent according to this plot: 6.8% percent of observations are fraudulent postings, compared to 4.8% in the entire dataset.

```{r, echo=FALSE, fig.height=5}
jobs_train %>%
  filter(str_detect(location, 'US,')) %>%
  ggplot(aes(x = as.factor(fraudulent))) +
  geom_bar() +
  geom_text(stat='count', aes(label=..count.., vjust=-.5)) +
  ggtitle("Fraudulent Versus Legitimate Job Postings, United States") +
  labs(x = "Fraudulent")
```

**5.** This plot shows that legitimate job postings have longer company profile descriptions, so we consider this variable to be important.
```{r, echo=FALSE}
jobs_train %>%
  ggplot(aes(x = as.factor(fraudulent), y = comp_prof_numchar)) +
  geom_boxplot() +
  ggtitle("Fraudulent versus Legitimate Job Postings; Number of Characters in Company Profile") +
  labs(x="Fraudulent?", y = "Number of Characters in Company Profile Description")
```

**6.** This plot shows that the same can't be said for job description character count; while the highest outliers are legitimate, the average number characters for fraudulent and legitimate job postings is similar.
```{r, echo=FALSE}
jobs_train %>%
  ggplot(aes(x = as.factor(fraudulent), y = description_numchar)) +
  geom_boxplot() +
  ggtitle("Fraudulent versus Legitimate Job Postings; Number of Characters in Job Description") +
  labs(x="Fraudulent?", y = "Number of Characters in Job Description")
```

These plots give us a sense of which variables may be effective to include in models, which are presented below.

# Building Models

We built 9 classification models to predict fraudulent job postings. It is important to note that we use **sampling="down"** in the trainControl function, which decreases the accuracies but provides more useful specificities and sensitivities by removing some of the data. Different classification models were used, including logistic models, LASSO, classification trees, and random forests. Models also differed in which variables were included, with some models containing as little as three variables and others containing all ten variables we found to be relevant. For each model, we computed a cross-validated accuracy, ran a confusion matrix, and analyzed ROC graphs and AUC quantities. Below is a table of our results for these models:

| Model | Description | CV Accuracy | Training Accuracy | AUC |
|-------|-------------|--------------|-------------|-------|
|jobs_log_model_1 |three binary variables|  .8106| .7995| .7578|
|jobs_log_model_2| three categorical variables|  .5339|  .5374| .7578|
|jobs_log_model_3| all relevant variables|  .7672| .7725| .8889|
|jobs_log_model_4| all relevant variables with some interaction terms | .7653 | .7755 | .8083 |
|jobs_lasso| lambda = 0.007564633 |  .7815| .7933| .892|
|**jobs_tree** | cp = 0 |  **.8429**| **.8334**|**.9076**|
|**jobs_randf**| mtry = 10 |  **.8661**| **.8786**| **.9572**|
|jobs_tree_select| cp = 0.003, removed three variables from first randf model|.830|.8468|.8888|
|jobs_randf_select| mtry=10, removed three variables from first randf model|.8570|.859|.9339|

In analyzing the accuracies and AUCs, **jobs_tree** and **jobs_randf** were the two models we picked as the best models. **jobs_randf** is a random forest model containing all ten relevant variables we discovered from our exploration of the data, and it has the highest accuracies and AUC out of any model we fit. **jobs_tree** is a classification tree model with the same ten variables, and it has a higher CV accuracy and AUC than the other classification tree model we fit, jobs_tree_select. Below, we show the model fitting code and other summary statistics and plots for our two best models:




```{r, eval=FALSE, echo=FALSE}
set.seed(253)

jobs_log_model_1 <- train(
    as.factor(fraudulent) ~ telecommuting + has_company_logo + has_questions,
    data = jobs_train,
    method = "glm",
    family = "binomial",
    trControl = trainControl(method = "cv", number = 5, sampling = "down"),
    metric = "Accuracy",
    na.action = na.omit
)

summary(jobs_log_model_1) %>% 
  coef() %>% 
  tidy() %>% 
  select(`.rownames`, Estimate) %>% 
  mutate(exp_coef = exp(Estimate))
```


```{r, echo=FALSE, eval=FALSE}
jobs_log_model_1$results$Accuracy
```


```{r, echo=FALSE, eval=FALSE}
confusionMatrix(data = predict(jobs_log_model_1, 
                               type = "raw"),
                reference = as.factor(jobs_train$fraudulent),
                positive = "1")
```


```{r, echo=FALSE, eval=FALSE}
#d = actual status, m = predicted probability
jobs_train %>% 
  mutate(PredStatus =  predict(jobs_log_model_1, type = "prob")$`1`) %>%
  ggplot(aes(d = as.numeric(fraudulent), m = PredStatus)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray")
```


```{r, echo=FALSE, eval=FALSE}
#roc(actual_class, predicted_probability)
jobs_train %>% 
  mutate(PredStatus =  predict(jobs_log_model_1, type = "prob")$`1`) %>%
  roc(fraudulent ~ PredStatus, data=.) %>% 
  auc()
```

```{r, echo=FALSE, eval=FALSE}
set.seed(253)

jobs_log_model_2 <- train(
    as.factor(fraudulent) ~ employment_type + required_experience + required_education,
    data = jobs_train,
    method = "glm",
    family = "binomial",
    trControl = trainControl(method = "cv", number = 5, sampling = "down"),
    metric = "Accuracy",
    na.action = na.omit
)

summary(jobs_log_model_2) %>% 
  coef() %>% 
  tidy() %>% 
  select(`.rownames`, Estimate) %>% 
  mutate(exp_coef = exp(Estimate))
```


```{r, echo=FALSE, eval=FALSE}
jobs_log_model_2$results$Accuracy
```


```{r, echo=FALSE, eval=FALSE}
confusionMatrix(data = predict(jobs_log_model_2, newdata = jobs_train,
                               type = "raw"),
                reference = as.factor(jobs_train$fraudulent),
                positive = "1")

# length(as.factor(ifelse(predict(jobs_log_model_2$finalModel, 
#                               type = "response") > 0.1, "1", "0")))

#length(as.factor(jobs_train$fraudulent))
```


```{r, echo=FALSE, eval=FALSE}
jobs_train %>% 
  mutate(PredStatus =  predict(jobs_log_model_2, newdata=jobs_train, type = "prob")$`1`) %>%
  ggplot(aes(d = as.numeric(fraudulent), m = PredStatus)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray")
```

```{r, echo=FALSE, eval=FALSE}
jobs_train %>% 
  mutate(PredStatus =  predict(jobs_log_model_1, type = "prob")$`1`) %>%
  roc(fraudulent ~ PredStatus, data=.) %>% 
  auc()
```



```{r, echo=FALSE, eval=FALSE}
set.seed(253)

jobs_log_model_3 <- train(
    as.factor(fraudulent) ~  description_numchar + comp_prof_numchar + domestic + employment_type + has_company_logo + n_miss_all + industry + required_education + required_experience + has_questions,
    data = jobs_train,
    method = "glm",
    family = "binomial",
    trControl = trainControl(method = "cv", number = 5, sampling = "down"),
    metric = "Accuracy",
    na.action = na.omit
)

summary(jobs_log_model_3) %>% 
  coef() %>% 
  tidy() %>% 
  select(`.rownames`, Estimate) %>% 
  mutate(exp_coef = exp(Estimate))
```


```{r, echo=FALSE, eval=FALSE}
jobs_log_model_3$results$Accuracy
```

```{r, echo=FALSE, eval=FALSE}
confusionMatrix(data = predict(jobs_log_model_3, newdata = jobs_train,
                               type = "raw"),
                reference = as.factor(jobs_train$fraudulent),
                positive = "1")
```

```{r, echo=FALSE, eval=FALSE}
jobs_train %>% 
  mutate(PredStatus =  predict(jobs_log_model_3, newdata=jobs_train, type = "prob")$`1`) %>%
  ggplot(aes(d = as.numeric(fraudulent), m = PredStatus)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray")
```

```{r, echo=FALSE, eval=FALSE}
jobs_train %>% 
  mutate(PredStatus =  predict(jobs_log_model_3, newdata = jobs_train, type = "prob")$`1`) %>%
  roc(fraudulent ~ PredStatus, data=.) %>% 
  auc()
```

```{r, echo=FALSE, eval=FALSE}
set.seed(253)

jobs_log_model_4 <- train(
    as.factor(fraudulent) ~  description_numchar + comp_prof_numchar + domestic + employment_type + has_company_logo*has_questions + n_miss_all + industry + required_education*required_experience,
    data = jobs_train,
    method = "glm",
    family = "binomial",
    trControl = trainControl(method = "cv", number = 5, sampling = "down"),
    metric = "Accuracy",
    na.action = na.omit
)
```


```{r, echo=FALSE, eval=FALSE}
jobs_log_model_4$results$Accuracy
```

```{r, echo=FALSE, eval=FALSE}
confusionMatrix(data = predict(jobs_log_model_4, newdata = jobs_train,
                               type = "raw"),
                reference = as.factor(jobs_train$fraudulent),
                positive = "1")
```

```{r, echo=FALSE, eval=FALSE}
jobs_train %>% 
  mutate(PredStatus =  predict(jobs_log_model_4, newdata=jobs_train, type = "prob")$`1`) %>%
  ggplot(aes(d = as.numeric(fraudulent), m = PredStatus)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray")
```

```{r, echo=FALSE, eval=FALSE}
jobs_train %>% 
  mutate(PredStatus =  predict(jobs_log_model_4, newdata = jobs_train, type = "prob")$`1`) %>%
  roc(fraudulent ~ PredStatus, data=.) %>% 
  auc()
```

```{r, echo=FALSE, eval=FALSE}
set.seed(253)

lambda_grid <- 10^seq(-4, -2, length = 100)

jobs_lasso <- train(
    as.factor(fraudulent) ~ telecommuting + has_company_logo + has_questions + employment_type + required_experience + required_education + n_miss_all + comp_prof_numchar + description_numchar + domestic + industry,
    data = jobs_train,
    method = "glmnet",
    family = "binomial",
    trControl = trainControl(method = "cv", number = 5, sampling = "down"),
    tuneGrid = data.frame(alpha = 1, 
                          lambda = lambda_grid),
    metric = "Accuracy",
    na.action = na.omit
)
```

```{r, echo=FALSE, eval=FALSE}
jobs_lasso$results %>% 
  ggplot(aes(x = lambda, y = Accuracy)) +
  geom_line() +
  scale_x_log10()
```

```{r, echo=FALSE, eval=FALSE}
jobs_lasso$bestTune$lambda
```

```{r, echo=FALSE, eval=FALSE}
jobs_lasso$results %>%
  arrange(desc(Accuracy)) %>%
  head(n = 1)
```

```{r, echo=FALSE, eval=FALSE}
confusionMatrix(data = predict(jobs_lasso, newdata = jobs_train, type = "raw"),
                reference = as.factor(jobs_train$fraudulent),
                positive = "1")
```

```{r, echo=FALSE, eval=FALSE}
jobs_train %>% 
  mutate(PredStatus =  predict(jobs_lasso, newdata=jobs_train, type = "prob")$`1`) %>%
  ggplot(aes(d = as.numeric(fraudulent), m = PredStatus)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray")
```

```{r, echo=FALSE, eval=FALSE}
jobs_train %>% 
  mutate(PredStatus =  predict(jobs_lasso, newdata = jobs_train, type = "prob")$`1`) %>%
  roc(fraudulent ~ PredStatus, data=.) %>% 
  auc()
```

**jobs_tree: Classification Tree with 10 Variables**
```{r}
set.seed(253)

jobs_tree <- train(
  as.factor(fraudulent) ~ description_numchar + comp_prof_numchar + domestic + employment_type + has_company_logo + n_miss_all + industry + required_education + required_experience + has_questions,
  data = jobs_train,
  method = "rpart",
  tuneGrid = data.frame(cp = seq(0, .05, length = 20)),
  trControl = trainControl(method = "cv", number = 5, sampling = "down"),
  metric = "Accuracy",
  na.action = na.omit
)
```

*CP vs. Accuracy Graph:*


```{r, echo=FALSE}
jobs_tree$results %>% 
  ggplot(aes(x = cp, y = Accuracy)) +
  geom_point() +
  geom_line()
```

*Best CP:*


```{r, echo=FALSE}
jobs_tree$bestTune$cp
```

*CV Accuracy:* 


```{r, echo=FALSE}
jobs_tree$results %>%
  arrange(desc(Accuracy)) %>%
  head(n = 1)
```

*Confusion Matrix:*


```{r, echo=FALSE}
confusionMatrix(data = predict(jobs_tree, newdata = jobs_train,
                               type = "raw"),
                reference = as.factor(jobs_train$fraudulent),
                positive = "1")
```

*ROC:*


```{r, echo=FALSE}
jobs_train %>% 
  mutate(PredStatus =  predict(jobs_tree, newdata = jobs_train, type = "prob")$`1`) %>%
  ggplot(aes(d = as.numeric(fraudulent), m = PredStatus)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray")
```

*AUC:*


```{r, echo=FALSE}
jobs_train %>% 
  mutate(PredStatus =  predict(jobs_tree, newdata = jobs_train, type = "prob")$`1`) %>%
  roc(fraudulent ~ PredStatus, data=.) %>% 
  auc()
```

**jobs_randf: Random Forest with 10 Variables**
```{r}
set.seed(253)

jobs_randf <- train(
  as.factor(fraudulent) ~ description_numchar + comp_prof_numchar + domestic + employment_type + has_company_logo + n_miss_all + industry + required_education + required_experience + has_questions,
  data = jobs_train, 
  method = "rf",
  trControl = trainControl(method = "oob", sampling = "down"),
  tuneGrid = data.frame(mtry = 10),
  ntree = 100, 
  nodesize = 5, 
  metric = "Accuracy",
  na.action = na.omit
)
```

*Trees vs Error (Accuracy) Plot:*


```{r, echo=FALSE}
plot(jobs_randf$finalModel)
```

*CV Accuracy:*


```{r, echo=FALSE}
jobs_randf$results
```

*Confusion Matrix:*


```{r, echo=FALSE}
confusionMatrix(data = predict(jobs_randf, newdata = jobs_train, type = "raw"),
                reference = as.factor(jobs_train$fraudulent),
                positive = "1")
```


*ROC:*


```{r, echo=FALSE}
jobs_train %>% 
  mutate(PredStatus =  predict(jobs_randf, newdata = jobs_train, type = "prob")$`1`) %>%
  ggplot(aes(d = as.numeric(fraudulent), m = PredStatus)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray")
```

*AUC:*


```{r, echo=FALSE}
jobs_train %>% 
  mutate(PredStatus =  predict(jobs_randf, newdata = jobs_train, type = "prob")$`1`) %>%
  roc(fraudulent ~ PredStatus, data=.) %>% 
  auc()
```

```{r, echo=FALSE, eval=FALSE}
set.seed(253)

jobs_tree_select <- train(
  as.factor(fraudulent) ~ comp_prof_numchar + domestic + employment_type + has_company_logo + n_miss_all + industry  + has_questions,
  data = jobs_train,
  method = "rpart",
  tuneGrid = data.frame(cp = seq(0, .05, length = 20)),
  trControl = trainControl(method = "cv", number = 5, sampling = "down"),
  metric = "Accuracy",
  na.action = na.omit
)
```

```{r, echo=FALSE, eval=FALSE}
jobs_tree_select$results %>% 
  ggplot(aes(x = cp, y = Accuracy)) +
  geom_point() +
  geom_line()
```

```{r, echo=FALSE, eval=FALSE}
jobs_tree_select$bestTune$cp
```

```{r, echo=FALSE, eval=FALSE}
jobs_tree_select$results %>%
  arrange(desc(Accuracy)) %>%
  head(n = 1)
```

```{r, echo=FALSE, eval=FALSE}
confusionMatrix(data = predict(jobs_tree_select, newdata = jobs_train,
                               type = "raw"),
                reference = as.factor(jobs_train$fraudulent),
                positive = "1")
```

```{r, echo=FALSE, eval=FALSE}
jobs_train %>% 
  mutate(PredStatus =  predict(jobs_tree_select, newdata = jobs_train, type = "prob")$`1`) %>%
  ggplot(aes(d = as.numeric(fraudulent), m = PredStatus)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray")
```

```{r, echo=FALSE, eval=FALSE}
jobs_train %>% 
  mutate(PredStatus =  predict(jobs_tree_select, newdata = jobs_train, type = "prob")$`1`) %>%
  roc(fraudulent ~ PredStatus, data=.) %>% 
  auc()
```

```{r, echo=FALSE}
set.seed(253)

jobs_randf_select <- train(
  as.factor(fraudulent) ~ comp_prof_numchar + domestic + employment_type + has_company_logo + n_miss_all + industry + has_questions,
  data = jobs_train, 
  method = "rf",
  trControl = trainControl(method = "oob", sampling = "down"),
  tuneGrid = data.frame(mtry = 10),
  ntree = 100, 
  nodesize = 5, 
  metric = "Accuracy",
  na.action = na.omit
)
```

```{r, echo=FALSE, eval=FALSE}
plot(jobs_randf_select$finalModel)
```

```{r, echo=FALSE, eval=FALSE}
jobs_randf_select$results
```

```{r, echo=FALSE, eval=FALSE}
confusionMatrix(data = predict(jobs_randf_select, newdata = jobs_train, type = "raw"),
                reference = as.factor(jobs_train$fraudulent),
                positive = "1")
```

```{r, echo=FALSE, eval=FALSE}
jobs_train %>% 
  mutate(PredStatus =  predict(jobs_randf_select, newdata = jobs_train, type = "prob")$`1`) %>%
  ggplot(aes(d = as.numeric(fraudulent), m = PredStatus)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray")
```

```{r, echo=FALSE, eval=FALSE}
jobs_train %>% 
  mutate(PredStatus =  predict(jobs_randf_select, newdata = jobs_train, type = "prob")$`1`) %>%
  roc(fraudulent ~ PredStatus, data=.) %>% 
  auc()
```

To test our best two models, we then refit each model on the testing data instead of the training data.

# Running the Best Models on Test Data

**Classification Tree Model: Test Data**
```{r}
set.seed(253)

jobs_tree_test <- train(
  as.factor(fraudulent) ~ description_numchar + comp_prof_numchar + domestic + employment_type + has_company_logo + n_miss_all + industry + required_education + required_experience + has_questions,
  data = jobs_test,
  method = "rpart",
  tuneGrid = data.frame(cp = seq(0, .05, length = 20)),
  trControl = trainControl(method = "cv", number = 5, sampling = "down"),
  metric = "Accuracy",
  na.action = na.omit
)
```

*CV Accuracy:*


```{r, echo=FALSE}
jobs_tree_test$results %>%
  arrange(desc(Accuracy)) %>%
  head(n = 1)
```

*AUC:*


```{r, echo=FALSE}
jobs_test %>% 
  mutate(PredStatus =  predict(jobs_tree_test, newdata = jobs_test, type = "prob")$`1`) %>%
  roc(fraudulent ~ PredStatus, data=.) %>% 
  auc()
```


**Random Forest Model: Test Data**
```{r}
set.seed(253)

jobs_randf_test <- train(
  as.factor(fraudulent) ~ description_numchar + comp_prof_numchar + domestic + employment_type + has_company_logo + n_miss_all + industry + required_education + required_experience + has_questions,
  data = jobs_test, 
  method = "rf",
  trControl = trainControl(method = "oob", sampling = "down"),
  tuneGrid = data.frame(mtry = 10),
  ntree = 100, 
  nodesize = 5, 
  metric = "Accuracy",
  na.action = na.omit
)
```

*CV Accuracy:*


```{r, echo=FALSE}
jobs_randf_test$results
```

*AUC:*


```{r, echo=FALSE}
jobs_test %>% 
  mutate(PredStatus =  predict(jobs_randf_test, newdata = jobs_test, type = "prob")$`1`) %>%
  roc(fraudulent ~ PredStatus, data=.) %>% 
  auc()
```

| Model | Description | CV Accuracy | AUC |
|-------|-------------|--------------|----|
|jobs_tree_test | cp = 0 |  .8432| .8162|
|**jobs_randf_test**| mtry = 10 |  **.8525**| **.9576**|

# Conclusion
**The random forest model *jobs_randf* is the best model for accurately predicting fraudulent job postings. It had the highest accuracies and AUC quantities of any model we fit on both the training and testing data**

By exploring the data and fitting models, we learned a few things about predicting fraudulent job postings:

- job postings with longer descriptions and company profiles are more likely to be legitimate
- job postings containing questions and company logos are more likely to be legitimate
- job postings in the United States are slightly more likely to be fraudulent
- legitimacy varies by employment level, required experience and education, and industry of a job
- the more robust a job posting is (i.e. the more information it contains), the more likely it is to be legitimate.

Our best models were random forest and classification tree models, but these are much harder to interpret than logistic models. We can easily interpret logistic models by using exponentiated coefficients, while trees are more complicated. In our specific study, we weighted accuracy of the model over interpretability due to the small amount of fraudulent postings. Further studies with this dataset could explore more combinations of variables to fit logistic models that have similar or greater accuracies than our classification tree and random forest models.

Overall, this report can be informative to any reader who surfs the internet and is wary about fraudulent job postings or websites or emails. Our results point out a few key relationships that can inform and warn people about the signs of fraudulent postings on the internet.
